\documentclass[12pt,leqno]{amsart}
\pagestyle{plain}
\usepackage{latexsym,amsmath,amssymb}
%\usepackage[notref,notcite]{showkeys} 

\setlength{\oddsidemargin}{1pt}
\setlength{\evensidemargin}{1pt}
\setlength{\marginparwidth}{30pt} % these gain 53pt width
\setlength{\topmargin}{1pt}       % gains 26pt height
\setlength{\headheight}{1pt}      % gains 11pt height
\setlength{\headsep}{1pt}         % gains 24pt height
%\setlength{\footheight}{12 pt} 	  % cannot be changed as number must fit
\setlength{\footskip}{24pt}       % gains 6pt height
\setlength{\textheight}{650pt}    % 528 + 26 + 11 + 24 + 6 + 55 for luck
\setlength{\textwidth}{460pt}     % 360 + 53 + 47 for luck



\def\dsp{\def\baselinestretch{1.35}\large
\normalsize}
%%%%This makes a double spacing. Use this with 11pt style. If you
%%%%want to use this just insert \dsp after the \begin{document}
%%%%The correct baselinestretch for double spacing is 1.37. However
%%%%you can use different parameter.


\def\U{{\mathcal U}}









\begin{document}
\bigskip
\centerline{\bf{Orthogonal Projections}}
\centerline{Connor Finucane}
\bigskip
{\bf $\newline$ Definition:} For $S$ a subspace of $\mathbb{R}^n$ with a given basis:  $\{\mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_k\}$ and a point $\mathbf{g} \in \mathbb{R}^n$ then $\mathbf{a} \in S$ is the orthogogonal projection of $\mathbf{g}$ onto $S$ if:
$$\forall i\leq k \ \  \ \ (\mathbf{g} - \mathbf{a})\cdot\mathbf{u}_i = 0 $$ 
$\newline$
{\bf Lemma 1: } For a vector sub space $S$ of $\mathbb{R}^n$ with a given basis: $\{\mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_k\}$ if some vector $\mathbf{v} \in S$ is such that for all $i \leq k \ \ \mathbf{v} \cdot \mathbf{u}_i = 0$ then $\mathbf{v} = \mathbf{0}$
\begin{proof}
Since $\mathbf{v} \in S$ we can write:
$$\mathbf{v} = t_1\mathbf{u}_1 + t_2\mathbf{u}_2 + \dots + t_k\mathbf{u}_k$$
It is sufficient to show that the magnitude of $\mathbf{v}$ is 0:
$$|\mathbf{v}|^2 = \mathbf{v}\cdot\mathbf{v} = \mathbf{v}\cdot\left(t_1\mathbf{u}_1 + t_2\mathbf{u}_2 + \dots + t_k\mathbf{u}_k\right) = t_1\mathbf{u}_1 \mathbf{v} +  t_2\mathbf{u}_2\mathbf{v} + \dots +  t_k\mathbf{u}_k\mathbf{v} = 0 $$
$$\iff |\mathbf{v}| = 0 $$
$$\iff \mathbf{v} = 0 $$
\end{proof}

$\newline$ 
{\bf Proposition 1: } If an orthogonal projection of $\mathbf{g}$ onto $S$ exists it is unique
\begin{proof} Let $\mathbf{a}$ and $\mathbf{b}$ be orthogonal projections of $\mathbf{g}$ onto $S$ 
\newline
\newline By defintion for an arbitrary $i \leq k$ we have: 
$$0 = (\mathbf{g} - \mathbf{a})\cdot\mathbf{u}_i = (\mathbf{g} - \mathbf{a} + \mathbf{b} - \mathbf{b})\cdot\mathbf{u}_i = (\mathbf{g} - \mathbf{b} - (\mathbf{a} - \mathbf{b}) )\cdot\mathbf{u}_i $$
$$= (\mathbf{g} - \mathbf{b})\cdot\mathbf{u}_i - (\mathbf{a} -\mathbf{b})\cdot\mathbf{u}_i = - (\mathbf{a} -\mathbf{b})\cdot\mathbf{u}_i $$ 
$$\iff (\mathbf{a} -\mathbf{b})\cdot\mathbf{u}_i = 0$$
Since $(\mathbf{a} - \mathbf{b}) \in S$ we can apply lemma 1:
$$\Rightarrow (\mathbf{a} - \mathbf{b}) = 0 $$
$$\iff \mathbf{a} = \mathbf{b} $$
\end{proof}
$\newline$
{\bf Proposition 2: } If $\mathbf{a} \in S$ is an orthogonal projection of $\mathbf{g}$ relative to a basis: $\{\mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_k\}$ then it is also orthogonal relative to any other basis
\begin{proof}
suppose that $\{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k\}$ is also a basis of $S$ and take an arbitrary $\mathbf{v}_i$
$$(\mathbf{g} - \mathbf{a})\cdot \mathbf{v}_i = (\mathbf{g} - \mathbf{a})\cdot(t_1 \mathbf{u}_1 + t_2\mathbf{u}_2 + \dots + t_k\mathbf{u}_k) \ \ \because \ \mathbf{v}_i \in S$$
$$ = t_1(\mathbf{g} - \mathbf{a})\cdot \mathbf{u}_1 + t_2(\mathbf{g} - \mathbf{a})\cdot\mathbf{u}_2 + \dots + t_k(\mathbf{g} - \mathbf{a})\cdot\mathbf{u}_k = 0 $$
$$ \Rightarrow (\mathbf{g} - \mathbf{a})\cdot\mathbf{v}_i = 0 		$$
\end{proof}
$\newline$
{\bf Proposition 3: } The given definition is equivalent to saying that the orthogonal projection of $\mathbf{a}$ onto $S$ (denoted $\mathbf{g}$) then $(\mathbf{g}-\mathbf{a})$ is orthogonal to every vector in $S$.
\begin{proof}
($\Rightarrow$) Fixing an arbitrary $\mathbf{v} \in S$ we know that $\mathbf{v}$ can be written as $\sum_{i=1}^k a_i\mathbf{u}_i$ for each $\mathbf{u}_i$ being a basis vector of $S$.  Therefore:
$$ (\mathbf{g} - \mathbf{a})\cdot \mathbf{v} =(\mathbf{g} - \mathbf{a})\cdot \sum_{i=1}^k a_i\mathbf{u}_i = \sum_{i=1}^k (\mathbf{g} - \mathbf{a})\cdot a_i\mathbf{u}_i = 0 $$
$(\Leftarrow)$ This is trivial because if $(\mathbf{g} - \mathbf{a})$ is orthogonal to all vectors in $S$ then in particular it is perpendicular to basis vectors of $S$.
\end{proof}
$\newline$
Given the results of propositions 1 and 2, orthogonal projections are independant of choice of basis vectors and are unique.  Therefore, the following notation will be introduced for $\mathbf{a}$ being $\mathbf{g}$'s orthogonal projection onto $S$:
$$ P_S(\mathbf{g}) = \mathbf{a} $$
$\newline$
{\bf Proposition 4: } If a point $\mathbf{g}$ is included in $S$ then its projection onto $S$ is itself.
\begin{proof} The proof is trivial since if a point exists which satisfies the first definition then the point is unique.  If we consider $\mathbf{g}$ itself then:
$$ (\mathbf{g} - \mathbf{g}) = \mathbf{0} $$
$$ \Rightarrow \forall i \leq k \ \ \mathbf{u}_i \cdot (\mathbf{g} - \mathbf{g}) = \mathbf{0} $$
And $\mathbf{g}$ is its own projection.  This also works in reverse in a sense because, if a projection of a point is calculated to be in exactly the same location as the point itself then the point must have been in $S$ in the first place!  I use this result to determine whether certain points are included in a subspace in the program that accompanies this.
$\newline$
\end{proof}
\noindent {\bf Theorem 1}  If a point $\mathbf{a} \in S$ minimizes the distance between $\mathbf{g}$ and the space $S$ then $P_S(\mathbf{g}) = \mathbf{a}$
\begin{proof} We can represent vectors in $S$ as parameterized in $k$ variables where $k$ is the dimension of the basis of $S$.  Therefore distances between $S$ and $\mathbf{g}$ can be written as:
$$ d(t_1,t_2,\dots, t_k) = ||t_1\mathbf{u}_{1} + t_2\mathbf{u}_{2} + \dots + t_k\mathbf{u}_{k} - \mathbf{g}|| $$
Since $d$ is always positive it suffices to consider $d^2$ when talking about local extrema.  Which can be written as:
$$ (t_1\mathbf{u}_{1} + t_2\mathbf{u}_{2} + \dots + t_k\mathbf{u}_{k} - \mathbf{g})\cdot (t_1\mathbf{u}_{1} + t_2\mathbf{u}_{2} + \dots + t_k\mathbf{u}_{k} - \mathbf{g}) $$
Since the product rule holds for the dot product or inner product we can write the derivate of $d^2$ with respect to the variable $t_i$ as:
$$ 2\mathbf{u}_i\cdot(t_1\mathbf{u}_{1} + t_2\mathbf{u}_{2} + \dots + t_k\mathbf{u}_{k} - \mathbf{g})  $$
To have a minimum we must have that:
$$ \forall i \leq k  \ \ \frac{\partial \, d^2}{\partial \, t_i} = 0 $$
$$ \forall i \leq k \ \ 2\mathbf{u}_i\cdot(t_1\mathbf{u}_{1} + t_2\mathbf{u}_{2} + \dots + t_k\mathbf{u}_{k} - \mathbf{g}) = 0 $$
$$ \iff \forall i \leq k \ \ \mathbf{u}_i\cdot(t_1\mathbf{u}_{1} + t_2\mathbf{u}_{2} + \dots + t_k\mathbf{u}_{k} - \mathbf{g})  = 0$$
Therefore $\mathbf{a}$ satisfies this then it is $\mathbf{g}$'s orthogonal projection by my original definition.
\end{proof}
$\newline$
Notice that if if we allow points in $S$ to be written as linear combinations of $S$'s basis $\mathbf{u}_i$ we can calculate the projection point by solving the system of equations:
$$ \mathbf{u}_1\cdot(t_1\mathbf{u}_{1} + t_2\mathbf{u}_{2} + \dots + t_k\mathbf{u}_{k} - \mathbf{a}) = 0 $$
$$ \cdots $$
$$ \mathbf{u}_k\cdot(t_1\mathbf{u}_{1} + t_2\mathbf{u}_{2} + \dots + t_k\mathbf{u}_{k} - \mathbf{a}) = 0 $$
Which is equivalent to:
$$ \mathbf{u}_1 \cdot t_1\mathbf{u}_{1} + \mathbf{u}_1 \cdot t_2\mathbf{u}_{2} + \dots + \mathbf{u}_1 \cdot t_k\mathbf{u}_{k} = \mathbf{u}_1 \cdot \mathbf{a} $$
$$ \cdots $$
$$ \mathbf{u}_k \cdot t_1\mathbf{u}_{1} + \mathbf{u}_k \cdot t_2\mathbf{u}_{2} + \dots + \mathbf{u}_k \cdot t_k\mathbf{u}_{k} = \mathbf{u}_k \cdot \mathbf{a} $$
Which is again equivalent to solving the matrix equation:
$$ \begin{bmatrix}
	\mathbf{u}_1 \cdot \mathbf{u}_1  & \mathbf{u}_1 \cdot \mathbf{u}_2 & \dots & \mathbf{u}_1 \cdot \mathbf{u}_k \\
	\mathbf{u}_2 \cdot \mathbf{u}_1  & \mathbf{u}_2 \cdot \mathbf{u}_2 & \dots & \mathbf{u}_2 \cdot \mathbf{u}_k \\
	\cdots & \cdots & \cdots & \cdots \\
	\mathbf{u}_k \cdot \mathbf{u}_1 & \mathbf{u}_k \cdot \mathbf{u}_2 & \dots & \mathbf{u}_k \cdot \mathbf{u}_k 
	\end{bmatrix}
	\begin{bmatrix}
	t_1 \\
	t_2 \\
	\dots \\
	t_k
	\end{bmatrix}	
	= 
	\begin{bmatrix}
	\mathbf{u}_1 \cdot \mathbf{a} \\
	\mathbf{u}_2 \cdot \mathbf{a} \\
	\dots \\
	\mathbf{u}_k \cdot \mathbf{a}
	\end{bmatrix}
 $$
Taking for granted that this matrix is invertable we can calculate $P_S(\mathbf{g})$'s coordinates with respect to the chosen basis as:
$$ \begin{bmatrix}
	t_1 \\
	t_2 \\
	\dots \\
	t_k
	\end{bmatrix}	
	= 
	\begin{bmatrix}
	\mathbf{u}_1 \cdot \mathbf{u}_1  & \mathbf{u}_1 \cdot \mathbf{u}_2 & \dots & \mathbf{u}_1 \cdot \mathbf{u}_k \\
	\mathbf{u}_2 \cdot \mathbf{u}_1  & \mathbf{u}_2 \cdot \mathbf{u}_2 & \dots & \mathbf{u}_2 \cdot \mathbf{u}_k \\
	\cdots & \cdots & \cdots & \cdots \\
	\mathbf{u}_k \cdot \mathbf{u}_1 & \mathbf{u}_k \cdot \mathbf{u}_2 & \dots & \mathbf{u}_k \cdot \mathbf{u}_k 
	\end{bmatrix} ^{-1}
	\begin{bmatrix}
	\mathbf{u}_1 \cdot \mathbf{a} \\
	\mathbf{u}_2 \cdot \mathbf{a} \\
	\dots \\
	\mathbf{u}_k \cdot \mathbf{a}
	\end{bmatrix}
$$ 

\noindent This is the method employed by my program to calculate projections!

\end{document}
$$\Rightarrow \mathbf{a} = a_1\mathbf{u}_1 + \dots + a_k\mathbf{u}_k  \text{  and  } \mathbf{b} = b_1\mathbf{u}_1 + \dots + b_k\mathbf{u}_k $$
Consider:
$$\mathbf{a} - \mathbf{b} = (a_1 - b_1)\mathbf{u}_1 + \dots + (a_k - b_k)\mathbf{u}_k$$
For every $i$ let $z_i := a_i - b_i$ and we can write:
$$\mathbf{a} - \mathbf{b} = z_1\mathbf{u}_1 + \dots + z_k\mathbf{u}_k$$